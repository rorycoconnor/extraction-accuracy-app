This is not a spec, but some research I did about Agent that might be applicable to our applicaiton.  The research might NOT have accurate representaion of our architectrure since it made some assumptions.  B

Modern Agentic System for Optimizing Metadata Extraction in Box Optimizer
Introduction
Metadata extraction is a key challenge in enterprise content management, where accuracy directly affects compliance and workflow efficiency. Box Optimizer is envisioned as an intelligent system built on the Box platform that dramatically improves metadata extraction accuracy using an agentic approach. Unlike a single-pass extraction, an agentic system can iteratively reason, adjust, and retry extractions to achieve high fidelity results. This research-informed design leverages Box’s metadata template framework, the Box AI API (with its OCR pipeline and Large Language Models), and state-of-the-art LLM techniques to create a self-improving extraction workflow. The system supports multiple AI models (OpenAI GPT, Anthropic Claude, Google Gemini, xAI Grok, etc.), uses custom prompts per metadata field, and continuously evaluates outputs against a user-provided ground truth corpus. Based on these evaluations, it refines its prompts or strategies in successive attempts, employing advanced reasoning methods (e.g. chain-of-thought, self-reflection, and DSPy-style prompt optimization) to converge on accurate metadata. We present a developer-oriented design of this agent orchestration, including guidance on controlling per-field iterations with retry limits and confidence checks, managing performance via batching and model selection, providing real-time progress feedback, and integrating with Box AI API features (structured extraction endpoints, model overrides, and cost tracking via Box AI Units). The approach is forward-looking, incorporating recent advances in LLM agent frameworks and Box’s latest AI capabilities, to outline a flexible yet powerful system for metadata extraction in enterprise content workflows.
Box Metadata Templates and Document Processing Pipeline
Box provides a robust metadata template system that allows organizations to define structured fields (attributes) to tag files (e.g. invoice number, contract date, client name). These templates are created and managed in the Box Admin Console or via API, and they impose a consistent schema on filessupport.box.comsupport.box.com. The proposed system assumes that for each document type we want to process (invoices, contracts, etc.), an appropriate metadata template exists with the required fields. Using Box’s metadata API, Box Optimizer can fetch the template schema (field keys, types, etc.) and use it to drive extraction.
When a document is ingested, Box automatically generates a text representation for it. If the file is a PDF or image without embedded text, Box’s pipeline will perform OCR in the background to create a searchable text layermedium.com. This means the Box AI API can handle scanned documents seamlessly – the OCR step runs prior to LLM inference if needed, so the LLM works off extracted text behind the scenesmedium.com. The system can rely on this pipeline rather than implementing its own OCR, ensuring that even images or scans can yield metadata.
For initiating extraction, the Box AI API offers two modes: freeform and structured extraction. In freeform mode, the request prompt and output format are unconstrained text (useful for ad-hoc queries), whereas structured extraction expects a defined set of fields and returns key–value outputs in a structured JSON formatdeveloper.box.comdeveloper.box.com. Box Optimizer will primarily use the structured extraction endpoint for predictable outputs and direct metadata mapping. Structured extraction can be invoked by specifying either a metadata template (by template ID/scope) or an explicit list of fields with their detailsdeveloper.box.comdeveloper.box.com. Providing the template or field list defines the “schema” for the LLM, so it knows which fields to extract. Optionally, each field can include a description or a prompt hint guiding the modeldeveloper.box.comdeveloper.box.com – a crucial feature we leverage for per-field prompt customization (discussed later). Box’s new Enhanced Extract Agent is fully compatible with this structured approachdeveloper.box.comdeveloper.box.com, allowing us to tap into its advanced chain-of-thought reasoning for complex documents. In summary, Box Optimizer sits atop Box’s content store and metadata templates: it pulls in file text via Box’s OCR/text index pipeline, then uses the structured extract API to request specific metadata fields, taking advantage of Box’s defined schemas and powerful AI agents.
Multi-Model LLM Support in Box AI
A core design goal is to make the extraction model-agnostic and even model-optimized. The Box AI API acts as a unified interface to multiple large language models – including OpenAI’s GPT series, Anthropic’s Claude, Google’s Gemini, and emerging models like xAI’s Grok – which can be selected via configuration overridesdeveloper.box.comdeveloper.box.com. Box regularly updates default models for each endpoint, but developers can explicitly choose a model from the supported list to suit their use casedeveloper.box.com. Box Optimizer will expose this selection to users or system admins, so they can choose the inference model for extraction. For instance, a user might select OpenAI GPT-5.1 for its general reasoning strength, Claude 4.5 for long documents (given Claude’s large context window), Gemini 3 for multi-modal nuances or speed, or Grok 3 if cost is a concern and it performs adequatelydeveloper.box.com. This flexibility ensures the system can incorporate the latest model advancements and tailor to the organization's priorities (accuracy vs. speed vs. cost).
Under the hood, using a specific model simply means overriding the Box AI agent configuration in the API request. For example, the agent payload can specify an ai_agent with a particular model for the text processing (as shown in Box’s API docs)developer.box.com. The system might maintain a mapping of model choices: e.g. “Standard” (default model), “Advanced” (GPT-5 or Claude Opus), “Economy” (smaller model or GPT-5 mini), etc., to make selection easy. In an agentic context, multi-model support also enables dynamic strategy: the agent could switch models on the fly – for example, trying a faster, lower-cost model first, and if it’s not confident or accurate, escalating to a more powerful model for a second attempt. Such an approach capitalizes on cheaper inference for easy cases and reserves expensive calls for harder cases, aligning with Box’s goal to help customers optimize AI usage costssiliconangle.comsiliconangle.com. Because Box recently introduced Box AI Units as a usage-tracking mechanism that quantifies each AI call or token usage into a cost unitsiliconangle.com, our system can leverage that tracking to inform model selection (e.g., using fewer AI Units by default, but spending more units when needed for accuracy). The net effect is a model-optimized pipeline: always use an appropriate model for the task complexity to balance accuracy, performance, and cost.
Prompt Customization and Ground-Truth Integration
Each metadata field often has unique context in documents, so a one-size-fits-all prompt may not yield the best extraction accuracy. Box Optimizer addresses this by allowing user-stored prompt instructions per field. Concretely, when setting up a metadata field in the Box template (or within the Optimizer’s configuration UI), users can provide a custom prompt hint or description for that field. This text is injected into the structured extraction request (the prompt or fields[].prompt field in the API) to guide the LLMdeveloper.box.com. For example, a field “Invoice Number” might carry the prompt hint: “The invoice number is typically labeled ‘Invoice #’ or ‘No.’ at the top of the document”. Another field “Total Amount” might have: “Look for the final total due, usually at the bottom, in format $X,XXX”. These targeted hints help the model focus on the right cues in the text. The Box AI Extract API natively supports such per-field instructions – indeed, Box’s VP of Product notes that developers “can define custom fields and instructions for each extract AI agent, fine-tuning them for specific scenarios”siliconangle.com. By storing and reusing these prompts, the system builds domain expertise into the extraction agent.
To measure and continually improve accuracy, Box Optimizer integrates a Ground Truth Corpus – a collection of documents (or excerpts) with known correct metadata values for each field. This serves two purposes: First, it provides an evaluation benchmark. After the AI extracts metadata for a test document, the results can be automatically compared to the ground truth values. Because the metadata fields are structured, simple string or numeric comparisons often suffice for evaluation (with possible normalization, e.g. date formats). The system will compute metrics such as per-field accuracy (percentage of documents where the field was correctly extracted) and overall extraction accuracy. This evaluation can run in real-time for a given batch, or offline on a training set. Second, the ground truth enables prompt refinement and tuning. By analyzing where the model’s output deviated from the ground truth, the agent can attempt to improve. For instance, if the model often misses a particular field or confuses it with another, that indicates the prompt for that field may need adjustment (or the model choice needs revisiting). The agent can use the ground truth data to systematically optimize prompts, akin to how the DSPy framework optimizes prompts using labeled examplesibm.com. In DSPy’s approach, the system generates variations of the prompt and tests them against a metric on sample data, retaining changes that improve accuracyibm.com. Box Optimizer can adopt a similar strategy: treat the ground truth corpus as an “evaluation set” and the prompts as tunable parameters. Whenever users update the ground truth (e.g. by correcting extraction results or adding new labeled docs), the agent could run a prompt optimization routine in the background – essentially a feedback loop where the LLM itself might propose alternative phrasings or the system tries different prompt templates, then picks the one yielding highest accuracy on the known answers. This moves prompt engineering from guesswork to a data-driven process, in line with modern evaluation-led prompt development practices.
Importantly, integrating ground truth also means the system can auto-evaluate each extraction in production (if the ground truth for that document is available). If a document is part of a validation set or has been manually reviewed before, the agent knows the expected answers and can flag any discrepancy immediately. This closes the loop for continuous improvement: incorrect extractions can be flagged for prompt refinement or even simple re-runs with corrected approach, as detailed next.
Automated Accuracy Evaluation
A modern agentic system should not rely solely on user feedback to catch errors – it should proactively evaluate its own outputs. In Box Optimizer, after the LLM produces metadata values for a document, an Automatic Evaluation Module compares those outputs against available references. The most straightforward scenario is when the document is part of the ground truth set (e.g., during a testing phase or continuous evaluation mode): the module simply checks each field’s extracted value against the known true value. If they match exactly (or within an allowable tolerance for numeric fields, date formats, etc.), the extraction is marked correct. If not, it’s marked as a failure for that field. The system can aggregate these results into metrics like precision, recall, or simpler accuracy rate per field. These metrics provide concrete targets for the agent to optimize (e.g., “increase invoice number accuracy from 85% to 95% on the validation set”).
In cases where ground truth isn’t known (e.g., new production documents), the system may still perform sanity checks as a proxy for confidence. For instance, it can verify that the extracted value actually appears in the document text or is consistent with expected patterns (e.g., an extracted date is in a plausible range, an invoice total is the sum of line items). The agent might also leverage an LLM-as-a-judge technique for complex validation – essentially asking a second-pass LLM or the same model in a different mode to rate or justify the extraction. Recent advancements allow LLMs to act as impartial evaluators following a rubricmedium.commedium.com. In our context, a rubric might be as simple as “output is correct if it exactly matches a value in the doc that corresponds to the field’s description.” The model can be prompted to return a score or true/false on whether the extracted value is correct given the document content (this is feasible since the model can be given the document text, the extracted value, and the field definition). Such LLM-based judging provides a scalable way to spot likely errors even without explicit ground truthmedium.commedium.com. However, to keep things simpler and more deterministic, Box Optimizer will primarily use direct comparisons to ground truth where available, and basic heuristic checks otherwise, only falling back to an LLM judge for edge cases.
The outcome of the evaluation step is crucial: it feeds into the agent’s decision on whether to accept the result or to trigger a refinement loop. In essence, the system continuously asks “Did I get it right?” after each extraction. If yes, it proceeds; if not, it initiates corrective strategies as described next.
Agentic Iterative Refinement Process
At the heart of the system is an agent orchestration logic that can iterate on extraction attempts in a goal-directed manner. Rather than a single API call per file, Box Optimizer treats metadata extraction as a sequence of actions informed by feedback. We outline the orchestration at a high level:


Per-Field Iteration: The agent approaches each metadata field as a sub-task to be solved. This granular focus is beneficial because the agent can devote attention to one field at a time, reducing interference. In practice, the system might first attempt to extract all fields in one call (for efficiency) using the structured API on a document. It then evaluates each field’s result. For any field that failed or is low-confidence, the agent enters an iteration loop for that field on the same document. This could involve issuing a follow-up extraction request targeting only that field (using the fields parameter to request a subset) or a specialized prompt. By isolating fields that need work, we avoid redoing correct extractions unnecessarily – although the design must consider that some LLM outputs might interdepend (rare for most metadata fields, but if context is needed, the agent might include previously extracted values as known context in the prompt). A field-specific iteration might, for example, find that the “Total Amount” was mis-read; the agent could then prompt the model in a second try with a more explicit instruction like “The total amount is the last bolded number in the document” if that insight was gleaned from analyzing the doc layout.


Chain-of-Thought Reasoning: One powerful technique the agent can employ in retries is prompting the model to think step-by-step. Box’s Enhanced Extract Agent already leverages chain-of-thought reasoning to handle complex, long documentssupport.box.comsupport.box.com. We can emulate and extend this: for a difficult field, the agent can switch to a prompt format that asks the model to first “show its work” (invisible to end user) before final answer. For example, a two-step prompt might say: “Explain how you identify the contract end date, then give the date.” The reasoning output can help the agent verify if the model looked at the correct part of text. More importantly, chain-of-thought can improve accuracy by allowing the model to use intermediate reasoning steps to parse cluttered or ambiguous contentsupport.box.com. This does come at a cost of higher token usage and latency, so the agent should apply it selectively (perhaps only on fields or documents flagged as challenging). Recent research indicates that for very capable models, chain-of-thought does not always boost accuracy and can introduce overheadlovelytics.com. Thus, the agent might dynamically decide: if a straightforward attempt failed, then enable chain-of-thought mode on retry. This way we pay the cost only when needed.


Self-Reflection and Prompt Adjustment: Beyond just trying again, the agent can learn from a failure through reflection. In a reflection strategy, after a wrong extraction, the agent asks the model (or uses logic) to analyze why the output might have been wrong. For instance, if the extracted “Purchase Order Number” was actually the invoice number, perhaps the prompt was too vague (“order” vs “invoice”). The agent can then refine the prompt: e.g., clarify “Purchase Order Number (not the invoice number)”. Techniques from advanced agent systems inspire this: the Reflexion approach has an agent critique its mistakes and adjust actions accordingly, and frameworks like DSPy systematically try prompt variants when metrics aren’t metibm.com. Box Optimizer can implement a simplified version: it maintains a knowledge base of common extraction errors (possibly accumulated from past runs), and when a field fails, it checks if any known pattern matches. If yes, apply the known fix (like adding a specific keyword to the prompt or instructing the model to focus on a certain section of text). If no known fix, the agent might instruct the model in a meta-prompt: “The last attempt was incorrect because it returned X but the correct value is Y. Adjust your approach to find the correct value.” Using the LLM itself to adjust its strategy is somewhat experimental, but with models like GPT-5 and Claude 4.5, it’s plausible – they can consume the feedback and alter their output on the next try. In any case, the agent maintains retry limits (e.g. maximum 3 attempts per field) to avoid infinite loops. If after 3 tries with various strategies the field is still not correct, the agent will record a failure and move on. Those failures become valuable data points – possibly to be fed back into prompt optimization for future improvements.


Confidence Evaluation: Before deciding to retry, the agent should evaluate confidence. If a model output is likely wrong or low confidence, a retry is warranted; if the model shows high confidence or the field is simply not present in the document, additional attempts may be wasteful. Confidence can be inferred in a few ways: the model might output a special token or note (if we instruct it to say “Unknown” when unsure); the agent can check if the output string appears in the text (if not, it might be hallucinated, hence low confidence); or use the LLM judge approach to get a score. If confidence is high (or the field passes validation rules), the agent will not iterate further on that field. This confidence gating prevents unnecessary loops and focuses effort where it’s most needed.


Use of Multiple Agents/Tools: Agentic systems often have access to tools (e.g., search engines, calculators, etc.). In our context, the primary “tool” is the Box AI extract API itself, but we can envision internal tools: for example, a regex checker for fields like phone numbers or dates, or a knowledge lookup if the field must match some reference data. If a particular field consistently fails with pure LLM extraction, the agent could invoke a backup strategy – e.g., for a field “Contract ID” that follows a pattern like CID-XXXX, a simple regex on the text might find it. Building such fallback tools for specific fields can drastically improve accuracy with minimal cost, though it requires some development effort. The system design can include a hook for field-specific tool plugins. These run if LLM output is suspect or as a parallel path to cross-verify the LLM. For example, after the LLM proposes a contract ID, the agent’s regex tool also scans the text for anything matching the pattern; if a match is found that differs, the agent might replace or at least flag the discrepancy. This hybrid approach combines the flexibility of LLMs with the precision of deterministic methods for optimal results.


The iterative refinement loop, combining the elements above, aligns with best practices from cutting-edge LLM applications. Anthropic’s Claude, for instance, is known for its constitutional AI approach – it can self-critique and refine answers – which we mirror in the agent’s self-reflection step. Google’s Gemini is expected to have strong multimodal reasoning, which could help if future extraction needs to interpret images or layouts (though currently Box provides text). The design is forward-looking: as LLMs evolve with better planning and tool use (GPT-5 series and beyond), the agent framework can incorporate those advances (for example, using function-calling abilities of newer GPT models to let the LLM choose a regex tool itself). By iterating per field with guided retries, Box Optimizer moves beyond one-shot prompting, instead adopting an approach of “deploy, evaluate, adjust, repeat” which is the hallmark of agentic AI systems.
Performance and Resource Management
While iterative loops and advanced models improve accuracy, they also raise concerns about performance (throughput, latency) and cost (API usage). The design therefore includes several strategies to manage efficiency:


Batch Processing: The system should batch operations wherever possible. The Box AI structured extract endpoint allows multiple files to be processed in one API call by specifying an array of items (file IDs)developer.box.comdeveloper.box.com. If the use-case involves large volumes of documents, Box Optimizer can group files and send a batch request to extract the same fields from all of them in one go. The response will contain results per file. Batching reduces overhead from network calls and enables the LLM to potentially amortize understanding if documents are similar. However, very large batches might strain the LLM’s context window or lead to interference between documents. A pragmatic approach is to batch a moderate number of documents (or pages) that the model can handle at once – perhaps on the order of 5-10 pages per call, which Box currently allows for imagesmedium.com. If documents are lengthy, it might be safer to process one at a time to avoid context overflow (Box’s Enhanced Agent can handle 50+ page docs with chain-of-thoughtsupport.box.com, but multiple such docs in one prompt could exceed limits). Thus, the system may dynamically choose a batch size based on total content length, ensuring we don’t exceed model capacity.


Early Exits and Timeouts: The agent should cut its losses on hopeless cases to maintain throughput. If a field has hit max retries and still failed, the agent will mark it as failed and not spend further time. Similarly, if an entire document yields very poor results across many fields, the system could decide to skip further intensive attempts on that document (perhaps deferring it for human review later). We can implement a simple heuristic: if after one full round (all fields attempted) the overall document accuracy is below a threshold (say <50% fields correct) and we’re already at high model usage, then don’t engage in a lengthy iterative process for that one document – flag it for manual inspection. This prevents one problematic file from hogging resources. Additionally, placing a global timeout on processing a single document ensures the system keeps up with SLAs. For example, set a 60-second limit per document – if not all fields are done by then, exit the loop. The remaining fields can be left blank or tagged as incomplete. These timeouts and exits guarantee some worst-case bounds on performance.


Model Tiering for Complexity: Not all fields require an equal amount of AI power. The system can classify fields by complexity and choose lighter-weight methods for simpler ones. For instance, fields like “Document Date” or “Reference Number” might appear in predictable locations or formats. The agent could first attempt a quick regex or a call to a smaller model (e.g., an OpenAI GPT-5 mini model) to extract these. If that succeeds confidently, there’s no need to involve a large model. On the other hand, a field like “Summary of Findings” (a more free-form text) would clearly need a top-tier model to interpret context. By routing each field to an appropriate “inference path”, we minimize overkill. Box’s AI agent configuration itself can support some level of this: it defines a basic_text vs long_text modeldeveloper.box.com – one could configure the basic_text (used for short content) to a cheaper model, and long_text to an advanced model, letting the API auto-select based on content length. Additionally, Box’s mention of Standard vs Enhanced Extract agents hints at a two-tier approach: use the Standard agent (likely faster, cheaper model) for documents or fields deemed simple, and switch to the Enhanced (chain-of-thought, larger model) for those that are complexsupport.box.comsupport.box.com. The agent can estimate complexity by factors like document length, number of fields to extract, or known difficulty of the field (perhaps learned over time via accuracy stats).


Cost Tracking and AI Unit Budgeting: With Box AI Units tracking usage, the system can implement cost-awareness. For example, an admin could set a budget of X AI Units per day. The agent then plans its work accordingly: if it’s running low on budget, it might reduce iterations or switch to more cost-efficient models to stay within limits. Conversely, if budget headroom exists, the agent can afford extra attempts or use the highest-accuracy (but costly) model to maximize results. The system should log AI Unit consumption per document and per extraction run, giving users transparency on how refining a field two extra times translated into e.g. 0.5 additional AI Units. This aligns with Box’s goal of making AI usage transparent and predictable for customerssiliconangle.comsiliconangle.com. In practical terms, the agent can query the remaining AI unit quota (if an API or metric is available), or track from responses (the Box API might return units consumed per call). If a particular document is trending towards using too many units (say an unusually large contract taking multiple expensive model calls), the system might abort early or prompt the user to approve further spending. Such measures keep the solution economically viable.


In summary, performance management in Box Optimizer is about smart allocation of resources: batch where possible, stop when needed, use the smallest effective model, and always keep an eye on cost. These measures ensure that the advanced agentic techniques improve accuracy without making the system impractically slow or expensive.
Real-Time Status Feedback to Users
To maintain user trust and provide a good UX, Box Optimizer will include real-time status updates throughout the extraction process. As the agent works through a batch of documents and fields, it will emit progress events that the front-end (or CLI, if used) can display. Key aspects of status feedback include:


Progress Metrics: Users should see how many documents have completed extraction, how many are in progress, and how many are pending. Similarly, within a document, it can show the count of fields successfully extracted, fields still being worked on, and any that failed. For instance, a status line might read: “Batch 1: 30 of 50 documents done (60%). Current doc: 5/7 fields extracted, 2 retries in progress…”. This granular progress indicator keeps users in the loop, especially important since an agentic approach may take longer than a single-pass method.


Live Updates on Iteration: When the agent triggers retries for a field, the user could be informed (e.g., an entry in a log like “Retrying field ‘Amount Due’ for Document XYZ (attempt 2 of 3)”). This gives visibility into the agent’s autonomous actions and can be reassuring that it’s actively working to improve the output. If a field ultimately hits max attempts or fails, the UI can flag it (maybe with a warning icon and a tooltip “Could not confidently extract this field after 3 attempts”). Such transparency allows the user to step in if needed – perhaps by manually providing that metadata or adjusting the prompt for future runs.


Completion and Results Viewing: As soon as a document’s metadata extraction is complete (whether fully successful or with some failures), the user should be able to view the results. If integrated into the Box App interface, this might mean the metadata fields are populated in the file’s sidebar (for those that succeeded) and highlighted if any are missing or uncertain. The agent can also provide a summary report per document: “10 fields extracted, 9 high confidence, 1 low confidence (Field X).” Real-time feedback thus isn’t just about numbers, but also about the quality of outcome.


User Intervention Hooks: Although not deeply in scope for this design, it’s worth noting that real-time status can invite the user to intervene. For example, if a field consistently fails, the interface might allow the user to supply the correct value, which the system could treat as ground truth for learning. The design stops short of a full UI spec here, but recommends providing at least a status dashboard and possibly interactive elements for corrections or confirmations. This mixture of automation and human-in-the-loop can be powerful in enterprise settings.


From an implementation standpoint, providing real-time status means the agent processes must be asynchronous and report progress events. This could be achieved via WebSocket updates, a polling API that clients hit for status, or integration with Box’s notification framework. The system could maintain a state machine for each batch: listing documents in states like “Pending, Processing, Completed, Failed”. In fact, because Box metadata operations can be asynchronous (and Box has workflow tools like Relay), one could even leverage those to update statuses of files as metadata is written. However it’s implemented, visibility is key – users should never be left guessing what the system is doing, especially as agentic behaviors can be complex. By clearly showing counts of completed vs. total and highlighting any problem areas in real-time, the system ensures a transparent and user-friendly experience.
Utilizing Box AI API Endpoints and Features
Box Optimizer heavily relies on Box’s AI API, and careful use of its features will maximize effectiveness:


Structured Metadata Extraction Endpoint: As discussed, the POST /ai/extract_structured endpoint is ideal for our use case. We recommend using the metadata template option in this endpoint to tightly integrate with Box’s metadata system. The workflow would be: the admin sets up a metadata template with the desired fields in Box (via console or API), then our system references that template by templateKey and scope in the extract API calldeveloper.box.comdeveloper.box.com. This way, the output from the AI is directly aligned to an existing template and can be stored immediately. Alternatively, one can send a custom fields array (with keys, displayNames, types, etc.) in the requestdeveloper.box.comdeveloper.box.com – this is useful if we want to dynamically alter prompts or add/remove fields on the fly. In practice, either approach should end with the system calling the metadata instances API to save the returned values to the file’s metadata instance (unless the AI endpoint auto-saves them, which currently it does not – it just returns the values). Thus, after a successful extraction, Box Optimizer will call PUT /files/{id}/metadata/{template} to persist each field value.


AI Agent Overrides: To support multi-model selection and advanced agent behavior, the system will use the AI agent configuration overrides in the API. The developer documentation indicates one can retrieve the default agent config (GET /ai/configuration) and then modify it for the requestdeveloper.box.comdeveloper.box.com. For example, if the default uses Azure OpenAI GPT-4o, but we want Anthropic Claude for its longer context window, we set the agent’s model to AWS Claude 4 Opus (as per the supported model list)developer.box.com. There are also parameters for how the agent handles long text vs short text, as shown in the example configdeveloper.box.com. The design can include a library of preset configurations: e.g., HighAccuracyAgent (Claude 4.5 with chain-of-thought enabled), FastAgent (Gemini Flash model without CoT), etc. The user’s selection or the agent’s dynamic choice will swap in the appropriate override before making the API call. This ensures we take full advantage of Box’s pluggable AI backends.


Enhanced Extract vs Standard: Box offers an Enhanced Extract Agent (with CoT) for Enterprise Plus tierssupport.box.comsupport.box.com. We should allow toggling this mode. If the user has access and wants the best accuracy for complex docs, enabling Enhanced mode will leverage those chain-of-thought capabilities inherentlysupport.box.com. Under the hood, enabling it might simply mean choosing a model configuration that corresponds to the enhanced chain-of-thought model (for instance, maybe a specific prompt or an agent type indicator). The design will account for checking license/tier (so the system knows if Enhanced is available) and then using it appropriately.


Cost Metrics via Box AI Units: As noted, Box AI Units provide a unified measure of usage. The system should retrieve usage data either via a Box API or through the admin console exports to correlate actions with consumption. For example, after each extraction API call, if the response or headers include how many units were consumed, the agent can log that. We recommend building a cost dashboard where an admin can see “This run processed 100 files, used 250 AI Units, average 2.5 per file.” Such transparency, combined with the earlier described dynamic budgeting, will help in maintaining the system in production. If Box’s API offers an endpoint to query AI Unit usage (possibly in the admin reports), the system could periodically poll it to see overall usage trends and perhaps alert if nearing limits. The key point is that every iterative retry and every model selection has a cost implication, and Box Optimizer won’t treat that as an afterthought – it will actively measure and expose it, so there are no surprise bills. This aligns with the new mechanism in Box which “quantifies each interaction, whether querying content or extracting metadata, into discrete units” for easier cost forecastingsiliconangle.com.


Agent Extensibility (Box AI Studio): The design thus far focuses on our custom orchestration. It’s worth mentioning that Box AI Studio itself allows creation of custom agents. In the future, some logic might be offloaded to Box’s managed agents. For example, if Box AI Studio supports an agent that can do multi-step tool use, one could configure an agent there and call it via the API. However, given we want fine-grained control (especially for prompt tuning and dynamic behavior), implementing our own loop is likely more flexible. The system will nonetheless remain flexible to integrate with Box AI Studio endpoints if that evolves (for instance, a stored agent that encapsulates some prompts or uses a specific chain).


In providing these technical suggestions, we intentionally avoid prescribing one rigid implementation. The idea is to leverage Box’s building blocks – the structured extract API, metadata storage, AI model overrides, and usage metrics – in a way that best fits the organization’s needs. Developers can choose to implement the agent loop in a back-end service (listening for file upload events, then kicking off extraction), or as part of a workflow (Box Relay/Flows triggering the Optimizer), or even as an interactive app within Box’s UI. The recommendation is to modularize the components: a Extractor Module (handles API calls and immediate results), an Agent Logic Module (handles decisions on retries and prompt changes), an Evaluation Module (compares against ground truth or validation rules), and a Feedback Module (updates status and perhaps adjusts prompts globally). Each piece should be configurable and replaceable – e.g., to update how prompts are generated without altering the rest of the system. By adhering to Box’s API contracts and keeping business logic separate, the implementation can remain nimble and adapt to new techniques (such as new optimizers from DSPy or updated Box AI capabilities) down the line.
Conclusion
The proposed agentic system for Box Optimizer marries the strengths of Box’s content and metadata platform with cutting-edge LLM orchestration techniques to achieve high-accuracy metadata extraction. It uses Box’s metadata templates and OCR pipeline as a foundation, then layers on intelligent selection of AI models, per-field prompt engineering, and a closed-loop of evaluation and refinement inspired by the latest research (Claude 4.5’s self-improvement, GPT-5’s advanced reasoning, Google’s Gemini multi-strategy approach, and DSPy’s programmatic prompt tuning). The design emphasizes a forward-looking, flexible architecture: one that is resilient (able to correct itself when wrong), efficient (mindful of performance and cost), and transparent (keeping users informed in real time). By automatically adjusting to feedback and focusing effort where it yields the most gain, the system should significantly outperform a single-pass extraction in accuracy – all while integrating smoothly with Box’s AI API and keeping the implementation open for future enhancements. This approach reflects the emerging best practice in AI-powered applications: treat AI not as a one-shot service, but as an iterative agent that can be guided and improved continuouslyibm.comlovelytics.com. With this design, Box Optimizer can continuously learn from its mistakes, refine its prompts and strategies, and ultimately provide enterprise users with reliable, accurate metadata at scale, unlocking more value from their content with minimal manual effort.
Sources:


Box Developer Blog – “[NEW!] Box AI API updates” (Alex Novotny, 2025)medium.commedium.com


Box Support – “Announcing Box AI Enhanced Extract Agent” (Feb 2025)support.box.comsupport.box.com


Box Developer Documentation – “Extract metadata (structured) API Reference”developer.box.comdeveloper.box.com


Box Developer Documentation – “AI model overrides”developer.box.comdeveloper.box.com


SiliconANGLE – “Box adds agentic AI features...” (Feb 19, 2025)siliconangle.comsiliconangle.com


SiliconANGLE – (cont’d)siliconangle.com (Custom fields and instructions per agent)


IBM Prompt Engineering Guide – “What is DSPy?”ibm.com (Automated prompt optimization)


Lovelytics Blog – “Beyond Prompt Engineering: Agentic Workloads with DSPy”lovelytics.com (DSPy optimizers for prompt refinement)


Mayank Khulbe – “LLM as a Judge: Evaluating AI Agents with DSPy” (Medium, 2025)medium.commedium.com (LLM-based evaluation)


Box Support – “Customizing Metadata Templates”support.box.comsupport.box.com (Metadata template basics)

